{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 18s 9ms/step - loss: 0.2999 - accuracy: 0.9149 - val_loss: 0.1411 - val_accuracy: 0.9626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b606160f28>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.Sequential([\n",
    "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation=tf.nn.relu),\n",
    "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "  train_images,\n",
    "  train_labels,\n",
    "  epochs=1,\n",
    "  validation_data=(test_images, test_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\SSANTH~1\\AppData\\Local\\Temp\\tmpd7zzsqpg\\assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_models_dir = pathlib.Path(\"/tmp/mnist_tflite_models/\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84552"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\SSANTH~1\\AppData\\Local\\Temp\\tmp2c8z2853\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\SSANTH~1\\AppData\\Local\\Temp\\tmp2c8z2853\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23952"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()\n",
    "tflite_model_quant_file = tflite_models_dir/\"mnist_model_quant.tflite\"\n",
    "tflite_model_quant_file.write_bytes(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter_quant = tf.lite.Interpreter(model_path=str(tflite_model_quant_file))\n",
    "interpreter_quant.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = np.expand_dims(test_images[0], axis=0).astype(np.float32)\n",
    "\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "interpreter.set_tensor(input_index, test_image)\n",
    "interpreter.invoke()\n",
    "predictions = interpreter.get_tensor(output_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.imshow(test_images[0])\n",
    "template = \"True:{true}, predicted:{predict}\"\n",
    "_ = plt.title(template.format(true= str(test_labels[0]),\n",
    "                              predict=str(np.argmax(predictions[0]))))\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to evaluate the TF Lite model using \"test\" dataset.\n",
    "def evaluate_model(interpreter):\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Run predictions on every image in the \"test\" dataset.\n",
    "  prediction_digits = []\n",
    "  for test_image in test_images:\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    digit = np.argmax(output()[0])\n",
    "    prediction_digits.append(digit)\n",
    "\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  accurate_count = 0\n",
    "  for index in range(len(prediction_digits)):\n",
    "    if prediction_digits[index] == test_labels[index]:\n",
    "      accurate_count += 1\n",
    "  accuracy = accurate_count * 1.0 / len(prediction_digits)\n",
    "\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9626\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(interpreter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9624\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(interpreter_quant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
